爬虫是典型的 I/O 密集型任务,通过前面的课程，我们已经知道了可以通过多线程的方式为爬虫提速. 事实上，还有一种非常适合 I/O 密集型任务的并发编程方式，称之为**异步编程**，也可以称为异步 I/O。通过多个子程序相互协作的方式来提升 CPU 的利用率.

## 阻塞
阻塞状态指程序未得到所需计算资源时被挂起的状态。程序在等待某个操作完成期间，自身无法继续处理其他的事情.最典型的就是 I/O 中断（包括网络 I/O 、磁盘 I/O 、用户输入等）、休眠操作、等待某个线程执行结束
**eg**: 文件操作：程序读取或写入文件时，需要等待数据完成传输。
死锁（Deadlocks）：两个或多个线程因相互等待对方释放资源而永远无法继续。
等待用户输入，例如从控制台读取数据。

## 非阻塞
程序在等待某操作过程中，自身不被阻塞，可以继续处理其他的事情,仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态。
如果是 CPU 密集型任务，阻塞可能是由于 GIL。
如果是 I/O 密集型任务，阻塞更多与操作系统层的资源管理相关

## 同步
不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，我们称这些程序单元是同步执行的。例如给银行账户存钱的操作，在代码中使用了“锁”作为通信信号，让多个存钱操作强制排队顺序执行，这就是所谓的同步。
## 异步
不同程序单元在执行过程中无需通信协调，也能够完成一个任务，这种方式我们就称之为异步。例如，使用爬虫下载页面时，调度程序调用下载程序后，即可调度其他任务，而无需与该下载任务保持通信以协调行为。很显然，异步操作的完成时刻和先后顺序并不能确定。
同步与异步的关注点是**消息通信机制**，最终表现出来的是“有序”和“无序”的区别；阻塞和非阻塞的关注点是**程序在等待消息时状态**


# 生成器和协程
异步编程是一种“协作式并发”, 即通过**多个子程序相互协作的方式**提升 CPU 的利用率，从而减少程序在阻塞和等待中浪费的时间，最终达到并发的效果。我们可以将多个相互协作的子程序称为“协程”，它是实现异步编程的关键。先通过下面的代码，看看什么是生成器。
def calc_average():
    total, counter = 0, 0
    avg_value = None
    while True:
        curr_value = yield avg_value
        total += curr_value
        counter += 1
        avg_value = total / counter


def main():
    obj = calc_average()
    # 生成器预激活
    obj.send(None)
    for _ in range(5):
        print(obj.send(float(input())))


if __name__ == '__main__':
    main()


# 异步函数
async，await可以简化协程代码的编写，可以用更为简单的方式让多个子程序很好的协作起来。让display函数以异步的方式运转。
import asyncio
import time


async def display(num):
    await asyncio.sleep(1)
    print(num)


def main():
    start = time.time()
    objs = [display(i) for i in range(1, 10)]
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(objs))
    loop.close()
    end = time.time()
    print(f'{end - start:.3f}秒')


if __name__ == '__main__':
    main()

当协程遭遇 I/O 操作阻塞时，就会到事件循环中监听 I/O 操作是否完成，并注册自身的上下文以及自身的唤醒函数（以便恢复执行），之后该协程就变为阻塞状态。
上面的第12行代码创建了9个协程对象并放到一个列表中，第13行代码通过asyncio模块的get_event_loop函数获得了系统的事件循环，第14行通过asyncio模块的run_until_complete函数将协程对象挂载到事件循环上。
执行上面的代码会发现，9个分别会阻塞1秒钟的协程总共只阻塞了约1秒种的时间，因为阻塞的协程对象会放弃对 CPU 的占有而不是让 CPU 处于闲置状态，这种方式大大的提升了 CPU 的利用率。

aiohttp库

import asyncio
import re

import aiohttp //一个异步 HTTP 客户端库，用于发送 HTTP 请求。
from aiohttp import ClientSession

TITLE_PATTERN = re.compile(r'<title.*?>(.*?)</title>', re.DOTALL) //TITLE_PATTERN 是一个用于匹配 HTML 网页 <title> 标签内容的正则表达式。


async def fetch_page_title(url):
    async with aiohttp.ClientSession(headers={
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36',
    }) as session:  # type: ClientSession
        async with session.get(url, ssl=False) as resp: // 向目标 URL 发起 GET 请求。ssl=False 忽略了 SSL 证书验证。
            if resp.status == 200:
                html_code = await resp.text()
                matcher = TITLE_PATTERN.search(html_code)
                title = matcher.group(1).strip()
                print(title)


def main():
    urls = [
        'https://www.python.org/',
        'https://www.jd.com/',
        'https://www.baidu.com/',
        'https://www.taobao.com/',
        'https://git-scm.com/',
        'https://www.sohu.com/',
        'https://gitee.com/',
        'https://www.amazon.com/',
        'https://www.usa.gov/',
        'https://www.nasa.gov/'
    ]
    objs = [fetch_page_title(url) for url in urls] //将 fetch_page_title(url) 协程对象创建为列表 objs
    await asyncio.gather(*objs)  # gather 比 wait 更加适合这种批量协程任务的场景

if __name__ == '__main__':
    main()


## 并发编程在爬虫中的应用

“360图片”网站的页面使用了 Ajax 技术，这是很多网站都会使用的一种异步加载数据和局部刷新页面的技术。简单的说，页面上的图片都是通过 JavaScript 代码异步获取 JSON 数据并动态渲染生成的
例如，要获取“美女”频道的图片，我们可以请求如下所示的URL，其中参数ch表示请求的频道，=后面的参数值beauty就代表了“美女”频道，参数sn相当于是页码，0表示第一页（共30张图片），30表示第二页，60表示第三页
https://image.so.com/zjl?ch=beauty&sn=0


## 单线程版本爬虫
import os

import requests

def download_picture(url):
    filename = url[url.rfind('/') + 1:] //rfind() 是 Python 字符串的方法,+1 的作用是跳过 / 的位置，从下一个字符开始。: 表示从起始索引（url.rfind('/') + 1）到字符串结尾。
    resp = requests.get(url) //请求图片资源。
    if resp.status_code == 200:
        with open(f'images/beauty/{filename}', 'wb') as file: //创建文件夹路径并保存图片为二进制文件。
            file.write(resp.content)

def main():
    if not os.path.exists('images/beauty'): //检查路径是否存在。
        os.makedirs('images/beauty') //创建路径 images/beauty。
    for page in range(3): //生成 0 到 2 的数字，用于分页。
        resp = requests.get(f'https://image.so.com/zjl?ch=beauty&sn={page * 30}') //动态生成 API 请求 URL，其中 sn 参数控制分页（每页 30 张）。
        if resp.status_code == 200:
            pic_dict_list = resp.json()['list'] //解析响应数据为 JSON 格式，并提取 list 键的值。
            for pic_dict in pic_dict_list: //# 遍历每张图片的数据。
                download_picture(pic_dict['qhimg_url']) //从每张图片的字典中提取图片的 URL。

if __name__ == '__main__':
    main()
向 API 请求 3 页数据（每页包含 30 张图片的信息）。


## 异步I/O版本
import asyncio
import json
import os

import aiofile
import aiohttp


async def download_picture(session, url):
    filename = url[url.rfind('/') + 1:]
    async with session.get(url, ssl=False) as resp:
        if resp.status == 200:
            data = await resp.read()
            async with aiofile.async_open(f'images/beauty/{filename}', 'wb') as file:
                await file.write(data)


async def fetch_json():
    async with aiohttp.ClientSession() as session:
        for page in range(3):
            async with session.get(
                url=f'https://image.so.com/zjl?ch=beauty&sn={page * 30}',
                ssl=False
            ) as resp:
                if resp.status == 200:
                    json_str = await resp.text()
                    result = json.loads(json_str)
                    for pic_dict in result['list']:
                        await download_picture(session, pic_dict['qhimg_url'])


def main():
    if not os.path.exists('images/beauty'):
        os.makedirs('images/beauty')
    loop = asyncio.new_event_loop()  # 创建新的事件循环
    asyncio.set_event_loop(loop)  # 将其设置为当前线程的事件循环
    try:
        loop.run_until_complete(fetch_json())
    finally:
        loop.close()


if __name__ == '__main__':
    main()

## 多线程版本
import os
from concurrent.futures import ThreadPoolExecutor

import requests


def download_picture(url):
    filename = url[url.rfind('/') + 1:]
    resp = requests.get(url)
    if resp.status_code == 200:
        with open(f'images/beauty/{filename}', 'wb') as file:
            file.write(resp.content)


def main():
    if not os.path.exists('images/beauty'):
        os.makedirs('images/beauty')
    with ThreadPoolExecutor(max_workers=16) as pool:
        for page in range(3):
            resp = requests.get(f'https://image.so.com/zjl?ch=beauty&sn={page * 30}')
            if resp.status_code == 200:
                pic_dict_list = resp.json()['list']
                for pic_dict in pic_dict_list:
                    pool.submit(download_picture, pic_dict['qhimg_url'])


if __name__ == '__main__':
    main()

使用多线程和异步 I/O 都可以改善爬虫程序的性能，因为我们不用将时间浪费在因 I/O 操作造成的等待和阻塞上，而time命令的执行结果也告诉我们，单线程的代码 CPU 利用率仅仅只有12%，而多线程版本的 CPU 利用率则高达95%；单线程版本的爬虫执行时间约21秒，而多线程和异步 I/O 的版本仅执行了3秒钟。
另外，在运行时间差别不大的情况下，多线程的代码比异步 I/O 的代码耗费了更多的 CPU 资源，这是因为多线程的调度和切换也需要花费 CPU 时间。


### 抓取网页动态内容
全球约有四分之三的网站是通过JavaScript动态生成的，这就意味着在浏览器窗口中“查看网页源代码”时无法在HTML代码中找到这些内容，也就是说我们之前用的抓取数据的方式无法正常运转了。
解决这样的问题基本上有两种方案，一是获取提供动态内容的数据接口，这种方式也适用于抓取手机 App 的数据；另一种是通过自动化测试工具 Selenium 运行浏览器获取渲染后的动态内容。
对于第一种方案，我们可以使用浏览器的“开发者工具”或者更为专业的抓包工具（如：Charles、Fiddler、Wireshark等）来获取到数据接口

# Selenium
Selenium 是一个自动化测试工具，利用它可以驱动浏览器执行特定的行为，最终帮助爬虫开发者获取到网页的动态内容。简单的说，只要我们在浏览器窗口中能够看到的内容，都可以使用 Selenium 获取到，对于那些使用了 JavaScript 动态渲染技术的网站，Selenium 会是一个重要的选择。
  
  加载页面
from selenium import webdriver

# 创建Chrome浏览器对象
browser = webdriver.Chrome()
# 加载指定的页面

##   查找元素和模拟用户行为
接下来，我们可以尝试模拟用户在百度首页的文本框输入搜索关键字并点击“百度一下”按钮。在完成页面加载后，可以通过Chrome对象的find_element和find_elements方法来获取页面元素，Selenium 支持多种获取元素的方式，包括：CSS 选择器、XPath、元素名字（标签名）、元素 ID、类名等，前者可以获取单个页面元素（WebElement对象），后者可以获取多个页面元素构成的列表。
获取到WebElement对象以后，可以通过send_keys来模拟用户输入行为，可以通过click来模拟用户点击操作
from selenium import webdriver
from selenium.webdriver.common.by import B
browser = webdriver.Chrome() //这一行代码启动了一个 Chrome 浏览器实例，webdriver.Chrome() 会打开一个新的浏览器窗口。
browser.get('https://www.baidu.com/') //通过调用 browser.get() 方法，程序会让浏览器打开百度的主页。
# 通过元素ID获取元素, 这行代码通过元素的 ID 属性获取百度页面上的搜索框元素。百度页面上的搜索框元素的 ID 是 'kw'
kw_input = browser.find_element(By.ID, 'kw')
# 模拟用户输入行为, 使用 send_keys() 方法模拟用户输入行为。这里输入了字符串 'Python'，模拟用户在搜索框中键入文字。
kw_input.send_keys('Python')
# 通过CSS选择器获取元素
su_button = browser.find_element(By.CSS_SELECTOR, '#su')
# 模拟用户点击行为,使用 send_keys() 方法模拟用户输入行为。这里输入了字符串 'Python'，模拟用户在搜索框中键入文字。
su_button.click()



##  隐式等待和显式等待
网页上的元素可能是动态生成的，在我们使用find_element或find_elements方法获取的时候，可能还没有完成渲染，这时会引发NoSuchElementException错误。为了解决这个问题，我们可以使用隐式等待的方式，通过设置等待时间让浏览器完成对页面元素的渲染。
除此之外，我们还可以使用显示等待，通过创建WebDriverWait对象，并设置等待时间和条件，当条件没有满足时，我们可以先等待再尝试进行后续的操作
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions
from selenium.webdriver.support.wait import WebDriverWait

browser = webdriver.Chrome()
# 设置浏览器窗口大小
browser.set_window_size(1200, 800)
browser.get('https://www.baidu.com/')
# 设置隐式等待时间为10秒
browser.implicitly_wait(10)
kw_input = browser.find_element(By.ID, 'kw')
kw_input.send_keys('Python')
su_button = browser.find_element(By.CSS_SELECTOR, '#su')
su_button.click()
# 创建显示等待对象
wait_obj = WebDriverWait(browser, 10)
# 设置等待条件（等搜索结果的div出现）
wait_obj.until(
    expected_conditions.presence_of_element_located(
        (By.CSS_SELECTOR, '#content_left')
    )
)
# 截屏
browser.get_screenshot_as_file('python_result.png')

隐式等待是告诉 Selenium 在查找元素时，如果元素没有立即找到，就等待一段时间,
显式等待是针对某个具体条件进行的等待，通常在你想等待某个特定的事件或者条件发生时使用。隐式等待和显式等待不应该混用。这两种等待机制会相互影响，可能导致一些意外的行为。

等待条件	具体含义
title_is / title_contains	标题是指定的内容 / 标题包含指定的内容
visibility_of	元素可见
presence_of_element_located	定位的元素加载完成
visibility_of_element_located	定位的元素变得可见
invisibility_of_element_located	定位的元素变得不可见
presence_of_all_elements_located	定位的所有元素加载完成
text_to_be_present_in_element	元素包含指定的内容
text_to_be_present_in_element_value	元素的value属性包含指定的内容
frame_to_be_available_and_switch_to_it	载入并切换到指定的内部窗口
element_to_be_clickable	元素可点击
element_to_be_selected	元素被选中
element_located_to_be_selected	定位的元素被选中
alert_is_present	出现 Alert 弹窗


## 执行JavaScript代码
如果希望在浏览器窗口中加载更多的内容，可以通过浏览器对象的execute_scripts方法执行 JavaScript 代码来实现。我们在上面的代码中截屏之前加入下面的代码，这样就可以利用 JavaScript 将网页滚到最下方。
# 执行JavaScript代码,document.documentElement：这是指页面的根元素，通常是 <html> 元素。它表示整个网页的最外层容器。
scrollTop：这个属性表示页面滚动条的垂直位置，即距离页面顶部的像素数。默认情况下，这个属性表示文档的总高度，包括不可见部分（即已经滚动出视口外的部分）。换句话说，scrollHeight 是整个页面的实际高度。
browser.execute_script('document.documentElement.scrollTop = document.documentElement.scrollHeight')


## 使用 Selenium 从“360图片”网站搜索和下载图片
import os
import time
from concurrent.futures import ThreadPoolExecutor

import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

DOWNLOAD_PATH = 'images/'


def download_picture(picture_url: str):
    """
    下载保存图片
    :param picture_url: 图片的URL
    """
    filename = picture_url[picture_url.rfind('/') + 1:]
    resp = requests.get(picture_url)
    with open(os.path.join(DOWNLOAD_PATH, filename), 'wb') as file:
        file.write(resp.content)


if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)
browser = webdriver.Chrome()
browser.get('https://image.so.com/z?ch=beauty')
browser.implicitly_wait(10)
kw_input = browser.find_element(By.CSS_SELECTOR, 'input[name=q]')
kw_input.send_keys('苍老师')
kw_input.send_keys(Keys.ENTER)
for _ in range(10):
    browser.execute_script(
        'document.documentElement.scrollTop = document.documentElement.scrollHeight'
    )
    time.sleep(1)
imgs = browser.find_elements(By.CSS_SELECTOR, 'div.waterfall img')
with ThreadPoolExecutor(max_workers=32) as pool:
    for img in imgs:
        pic_url = img.get_attribute('src')
        pool.submit(download_picture, pic_url)



# 爬虫框架Scrapy
Scrapy 是基于 Python 的一个非常流行的网络爬虫框架，可以用来抓取 Web 站点并从页面中提取结构化的数据。Scrapy 的工作流程本质上是一个循环，从 Spider 发起请求到解析数据并存储，最终通过引擎协调各模块完成数据抓取任务。
Spider: 用户自定义的用来解析网页并抓取特定URL的类,简单的说就是用来定义特定网站的抓取和解析规则的模块。
Scrapy 的整个数据处理流程由引擎进行控制，通常的运转流程包括以下的步骤：

1. 引擎询问蜘蛛需要处理哪个网站，并让蜘蛛将第一个需要处理的 URL 交给它。

2. 引擎让调度器将需要处理的 URL 放在队列中。

3. 引擎从调度那获取接下来进行爬取的页面。

4. 调度将下一个爬取的 URL 返回给引擎，引擎将它通过下载中间件发送到下载器。

5. 当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎；如果下载失败了，引擎会通知调度器记录这个 URL，待会再重新下载。

6. 引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。

7. 蜘蛛处理响应并返回爬取到的数据条目，此外还要将需要跟进的新的 URL 发送给引擎。

8. 引擎将抓取到的数据条目送入数据管道，把新的 URL 发送给调度器放入队列中。

上述操作中的第2步到第8步会一直重复直到调度器中没有需要请求的 URL，爬虫就停止工作。

## 安装和使用Scrapy
安装：pip install scrapy
在命令行中使用scrapy命令创建名为demo的项目： scrapy startproject demo
切换到demo 目录，创建名为douban的蜘蛛程序：scrapy genspider douban movie.douban.com

eg: 爬取豆瓣电影 Top250 电影标题、评分和金句.
1. 在items.py的Item类中定义字段, Item 类是 Scrapy 提供的一个容器，用于定义爬取数据的结构。通过明确地定义字段，方便在爬取过程中将解析的数据存储在统一格式的对象中，确保数据的一致性。其他开发者可以一眼看出数据结构和需要爬取的字段是什么
import scrapy

class DoubanItem(scrapy.Item):
    title = scrapy.Field()  // 电影标题
    score = scrapy.Field()  // 评分
    motto = scrapy.Field()  // 电影简介或标语

这样，解析到的数据会存储成如下的结构化对象：
{
    'title': '肖申克的救赎',
    'score': '9.7',
    'motto': '希望让人自由。'
}

2. 修改spiders文件夹中名为douban.py 的文件，我们可以通过对Response对象的解析，获取电影的信息，我们可以使用 CSS 选择器进行页面解析。
如果还要生成后续爬取的请求，我们可以用yield产出Request对象。Request对象有两个非常重要的属性，一个是url，它代表了要请求的地址；一个是callback，它代表了获得响应之后要执行的回调函数。
import scrapy
from scrapy import Selector, Request
from scrapy.http import HtmlResponse

from demo.demo.items import MovieItem


class DoubanSpider(scrapy.Spider):
    name = 'douban' //爬虫名称，运行时使用 `scrapy crawl douban`
    allowed_domains = ['movie.douban.com'] //限定爬取的域名范围，防止访问不相关的网站

    def start_requests(self):
        for page in range(10):
            yield Request(url=f'https://movie.douban.com/top250?start={page * 25}')

    def parse(self, response: HtmlResponse): //Scrapy 爬虫的核心，用于解析响应 (response)。Scrapy 会自动把初始 URL (start_urls) 的响应传入 response 对象。
        sel = Selector(response) //创建一个选择器对象，便于解析 HTML。
        movie_items = sel.css('#content > div > div.article > ol > li') //通过 CSS 选择器定位到电影列表，选择 Top250 页面中每个电影条目的 <li> 元素。
        for movie_sel in movie_items: //遍历每个电影条目：
            item = MovieItem() 
            item['title'] = movie_sel.css('.title::text').extract_first()
            item['score'] = movie_sel.css('.rating_num::text').extract_first()
            item['motto'] = movie_sel.css('.inq::text').extract_first()
            yield item //将提取的电影信息包装为 MovieItem 对象并交给 Scrapy 管道处理（如保存到文件或数据库）。

        hrefs = sel.css('#content > div > div.article > div.paginator > a::attr("href")')
        for href in hrefs:
            full_url = response.urljoin(href.extract())
            yield Request(url=full_url)
