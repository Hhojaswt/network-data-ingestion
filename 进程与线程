# 线程和进程
进程是操作系统分配存储空间的基本单位，每个进程都有自己的地址空间、数据栈以及其他用于跟踪进程执行的辅助数据
一个进程还可以拥有多个执行线索，简单的说就是拥有多个可以获得 CPU 调度的执行单元，这就是所谓的线程。
并发通常是指同一时刻只能有一条指令执行，但是多个线程对应的指令被快速轮换地执行。
并行是指同一时刻，有多条指令在多个处理器上同时执行.


## 多线程编程
Python 标准库中threading模块的**Thread**类可以帮助我们非常轻松的实现多线程编程。
def main():
    threads = [
        Thread(target=download, kwargs={'filename': 'Python从入门到住院.pdf'}),
        Thread(target=download, kwargs={'filename': 'MySQL从删库到跑路.avi'}),
        Thread(target=download, kwargs={'filename': 'Linux从精通到放弃.mp4'})
    ]
    start = time.time()
    # 启动三个线程
    for thread in threads:
        thread.start()
    # 等待线程结束
    for thread in threads:
        thread.join()
    end = time.time()
    print(f'总耗时: {end - start:.3f}秒.')
直接使用Thread类的构造器就可以创建线程对象，而线程对象的start()方法可以启动一个线程。
线程启动后会执行target参数指定的函数，当然前提是获得 CPU 的调度；如果target指定的线程要执行的目标函数有参数，需要通过args参数为其进行指定，对于关键字参数，可以通过kwargs参数进行传入。


还可以通过继承Thread类并重写run()方法的方式来自定义线程，具体的代码如下所示。
class DownloadThread(Thread):

    def __init__(self, filename):
        self.filename = filename
        super().__init__()

    def run(self):
        start = time.time()
        print(f'开始下载 {self.filename}.')
        time.sleep(random.randint(3, 6))
        print(f'{self.filename} 下载完成.')
        end = time.time()
        print(f'下载耗时: {end - start:.3f}秒.')


def main():
    threads = [
        DownloadThread('Python从入门到住院.pdf'),
        DownloadThread('MySQL从删库到跑路.avi'),
        DownloadThread('Linux从精通到放弃.mp4')
    ]
    start = time.time()
    # 启动三个线程
    for thread in threads:
        thread.start()
    # 等待线程结束
    for thread in threads:
        thread.join()
    end = time.time()
    print(f'总耗时: {end - start:.3f}秒.')


if __name__ == '__main__':
    main()


## 使用线程池
利用线程池，可以提前准备好若干个线程，在使用的过程中不需要再通过自定义的代码创建和释放线程，而是直接复用线程池中的线程。Python 内置的concurrent.futures模块提供了对线程池的支持
def main():
    with ThreadPoolExecutor(max_workers=4) as pool:
        filenames = ['Python从入门到住院.pdf', 'MySQL从删库到跑路.avi', 'Linux从精通到放弃.mp4']
        start = time.time()
        for filename in filenames:
            pool.submit(download, filename=filename)
    end = time.time()
    print(f'总耗时: {end - start:.3f}秒.')


## 守护线程
守护线程会跟随主线程一起挂掉，而主线程的生命周期就是一个进程的生命周期。
def display(content):
    while True:
        print(content, end='', flush=True)
        time.sleep(0.1)


def main():
    Thread(target=display, args=('Ping', )).start()
    Thread(target=display, args=('Pong', )).start()
上面的代码中，我们将print函数的参数flush设置为True，这是因为flush参数的值如果为False，而print又没有做换行处理，就会导致每次print输出的内容被放到操作系统的输出缓冲区，直到缓冲区被输出的内容塞满，才会清空缓冲区产生一次输出。上述现象是操作系统为了减少 I/O 中断，提升 CPU 利用率做出的设定，为了让代码产生直观交互，我们才将flush参数设置为True，强制每次输出都清空输出缓冲区。

上面的代码运行起来之后是不会停止的，因为两个子线程中都有死循环，除非你手动中断代码的执行。但是，如果在创建线程对象时，将名为daemon的参数设置为True，这两个线程就会变成守护线程，那么在其他线程结束时，即便有死循环，两个守护线程也会挂掉，不会再继续执行下去，
def display(content):
    while True:
        print(content, end='', flush=True)
        time.sleep(0.1)


def main():
    Thread(target=display, args=('Ping', ), daemon=True).start()
    Thread(target=display, args=('Pong', ), daemon=True).start()
    time.sleep(5)
上面的代码，我们在主线程中添加了一行time.sleep(5)让主线程休眠5秒，在这个过程中，输出Ping和Pong的守护线程会持续运转，直到主线程在5秒后结束，这两个守护线程也被销毁，不再继续运行。

## 资源竞争
在编写多线程代码时，不可避免的会遇到多个线程竞争同一个资源（对象）的情况。可以使用锁机制，通过锁对操作数据的关键代码加以保护。Python 标准库的threading模块提供了Lock和RLock类来支持锁机制，这里我们不去深究二者的区别，建议大家直接使用RLock

class Account(object):
    """银行账户"""

    def __init__(self):
        self.balance = 0.0
        self.lock = RLock()//创建一个 递归锁 (RLock)，用于在多线程环境下保护账户余额的访问。

    def deposit(self, money):
        # 通过上下文语法获得锁和释放锁
        with self.lock://确保在执行以下代码时获得锁，并且在代码块执行完毕后自动释放锁。
            new_balance = self.balance + money//self.balance 是当前账户的余额。money 是存款的金额。将当前余额和存款金额相加，计算出新的余额 new_balance。
            time.sleep(0.01)
            self.balance = new_balance//将计算出的新的余额 new_balance 存储回账户余额 self.balance 中。


def main():
    """主函数"""
    account = Account()
    with ThreadPoolExecutor(max_workers=16) as pool:
        for _ in range(100):
            pool.submit(account.deposit, 1)
    print(account.balance)


## 创建进程
在 Python 中可以基于Process类来创建进程，虽然进程和线程有着本质的差别，但是Process类和Thread类的用法却非常类似。
在使用Process类的构造器创建对象时，也是通过target参数传入一个函数来指定进程要执行的代码，而args和kwargs参数可以指定该函数使用的参数值。
def sub_task(content, nums)://sub_task 函数是一个简单的任务函数，它会打印出当前进程的 PID 和名字，并执行一些基于参数的操作。
    # 通过current_process函数获取当前进程对象
    # 通过进程对象的pid和name属性获取进程的ID号和名字
    print(f'PID: {current_process().pid}')
    print(f'Name: {current_process().name}')
    # 通过下面的输出不难发现，每个进程都有自己的nums列表，进程之间本就不共享内存
    # 在创建子进程时复制了父进程的数据结构，三个进程从列表中pop(0)得到的值都是20
    counter, total = 0, nums.pop(0) //这里 nums 是一个列表，pop(0) 操作会从列表的开头移除一个元素并返回。由于进程是独立的，每个进程对 nums 的操作互不影响，因此每个子进程拿到的 total 值会是列表中的第一个元素。
    print(f'Loop count: {total}')
    sleep(0.5)
    while counter < total:
        counter += 1
        print(f'{counter}: {content}')
        sleep(0.01)


def main():
    nums = [20, 30, 40]
    # 创建并启动进程来执行指定的函数
    Process(target=sub_task, args=('Ping', nums)).start()
    Process(target=sub_task, args=('Pong', nums)).start()
    # 在主进程中执行sub_task函数
    sub_task('Good', nums)



## 多进程和多线程的比较
对于爬虫这类 I/O 密集型任务来说，使用多进程并没有什么优势；但是对于计算密集型任务来说，多进程相比多线程，在效率上会有显著的提升
import concurrent.futures

PRIMES = [
    1116281,
    1297337,
    104395303,
    472882027,
    533000389,
    817504243,
    982451653,
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419
] * 5


def is_prime(n):
    """判断素数"""
    for i in range(2, int(n ** 0.5) + 1)://求 n 的平方根的操作，等价于 math.sqrt(n),使用了 int() 函数将平方根的结果转换为整数, + 1 用于确保 range 生成的序列包含整数 int(n ** 0.5)
        if n % i == 0:
            return False
    return n != 1


def main():
    """主函数"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor: //ProcessPoolExecutor 是 concurrent.futures 模块中的一个类，用于管理进程池并执行并行计算。max_workers=16 指定进程池中最多允许创建 16 个进程
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)): //将一个可迭代对象 PRIMES 中的每个元素传递给函数 is_prime，并行地计算它们的结果。将 PRIMES 和 executor.map 的结果逐个配对，形成一个元组 (number, prime)。
            print('%d is prime: %s' % (number, prime))


if __name__ == '__main__':
    main()
多线程的代码只能让 CPU 利用率达到100%，这其实已经证明了多线程的代码无法利用 CPU 多核特性来加速代码的执行,
多进程的版本在我使用的这台电脑上，让 CPU 的利用率达到了将近400%，而运行代码时用户态耗费的 CPU 的时间（106.63秒）几乎是代码运行总时间（27.497秒）的4倍
多进程可以突破 GIL 的限制，充分利用 CPU 多核特性



## GIL（Global Interpreter Lock，全局解释器锁）是 Python 解释器（CPython）的一种机制。它是一把全局锁，确保同一时间只有一个线程可以执行 Python 字节码。换句话说，即使在多线程环境下，GIL 限制了 Python 只能使用一个线程运行 Python 代码。
### GIL 的作用
GIL 的主要目的是保护 Python 的底层数据结构（如对象引用计数）的线程安全：

Python 的内存管理系统（基于引用计数）不是线程安全的。
使用 GIL 确保多个线程在操作 Python 对象时不会引发竞争条件（race condition）。
GIL 是一种简单且直接的方式来避免数据竞争，而不需要在每个数据结构上加锁。
**解决方法可以使用 multiprocessing 模块,每个进程都有自己的 Python 解释器实例和 GIL，不会相互影响。**



## 进程间通信
由于多个进程之间不能够像多个线程之间直接通过共享内存的方式交换数据,子进程会复制父进程及其所有的数据结构，每个子进程有自己独立的内存空间，这也就意味着两个子进程中各有一个counter变量,各自运行.
要解决这个问题比较简单的办法是使用multiprocessing模块中的Queue类，它是可以被多个进程共享的队列
import time
from multiprocessing import Process, Queue

def sub_task(content, queue):
    counter = queue.get()
    while counter < 50:
        print(content, end='', flush=True)
        counter += 1
        queue.put(counter) // 将更新后的计数器放回队列
        time.sleep(0.01)
        counter = queue.get() // 取出最新的计数器值

def main():
    queue = Queue() //创建共享队列
    queue.put(0) //初始化计数器为 0
    p1 = Process(target=sub_task, args=('Ping', queue))
    p1.start()
    p2 = Process(target=sub_task, args=('Pong', queue))
    p2.start()
    while p1.is_alive() and p2.is_alive()://主进程等待子进程完成, 通过 p1.is_alive() 和 p2.is_alive() 检查子进程是否仍在运行。如果子进程已完成，则终止循环并向队列中插入终止信号（值为 50）
        pass
    queue.put(50) //终止信号，防止死锁

if __name__ == '__main__':
    main()
上面的代码通过Queue类的get和put方法让三个进程（p1、p2和主进程）实现了数据的共享，这就是所谓的进程间的通信，通过这种方式，当Queue中取出的值已经大于等于50时，p1和p2就会跳出while循环，从而终止进程的执行。
